# 协程调度

## 协程调度器

---
协程调度器概述：

- 协程调度器用于创建线程池，并从任务队列里不断取出任务交给线程执行

利用线程局部变量记录当前线程的调度器，以及调度协程

```cpp
// 当前线程的调度器，同一个调度器下的所有线程指同同一个调度器实例
static thread_local Scheduler* t_scheduler = nullptr;
// 当前线程的调度协程，每个线程都独有一份，包括caller线程
static thread_local Fiber* t_scheduler_fiber = nullptr;
```

- **主要API**

协程调度器用`star`方法创建线程池，然后将调度的协程传递给调度器，由调度器负责把这些协程一个一个消耗掉。
调度器所在的线程，也可以用来执行调度任务。甚至，调度器完全可以不创建新的线程，只使用`main`函数所在的线程来进行协程调度。

```cpp
Scheduler(size_t threads = 1 , bool use_caller = true , const std::string& name = "");
virtual ~Scheduler();
void start();//开始调度
void stop();//结束调度
//添加调度任务(thread 协程执行的线程id,-1标识任意线程)
template<class Fiber0rCb>
void schedule(Fiber0rCb fc , int thread = -1)；
protected:
    virtual void tickle();//提醒调度器有任务
    void run();//线程执行函数
    virtual bool stopping();//是否正在停止
    virtual void idle();//当无任务是进入idle协程
private:
    //调度任务定义
    struct FiberAndThread {
        Fiber::ptr fiber;
        std::function<void()> cb;
        /// 线程id
        int thread;
        ...
    };
```

调度器内部维护一个任务队列和一个调度线程池。开始调度后，线程池从任务队列里按顺序不断取任务执行。添加调度任务就是向任务队列添加任务。
调度线程可以包含主线程`（caller线程）`。当全部任务都执行完了，线程池停止调度，等新的任务进来。添加新任务后，通知线程池有新的任务进来了，线程池重新开始运行调度。停止调度时，各调度线程退出，调度器停止工作。

```cpp
//线程池
std::vector<Thread::ptr> m_threads;
//任务队列
std::list<FiberAndThread> m_fibers;
```

如果将主线程加入调度`(uese_caller = true)`，则要在主线程中创建调度协程
此时主线程的调度情况如下

```txt
    主线程 --> 调度协程 --> 任务协程1
                            |
                            |
               调度协程  <-- | 
                |
                | ---->任务协程2
                            |
                            |
    主线程 <---调度协程 ---  |
```

```cpp
Scheduler::Scheduler(size_t threads , bool use_caller , const std::string& name):m_name(name){
    ...
    if(use_caller){
        //主线程的主协程
        radixun::Fiber::GetThis();
        ...
        //调度协程执行函数
        m_rootFiber.reset(new Fiber(std::bind(&Scheduler::run , this) , 0 , true));
        ...
    }else{
        m_rootThread = -1;
    }
    m_threadCount = threads;
}
```

`start`方法调用后会创建调度线程池，线程数量由初始化时的线程数和`use_caller`确定。
每个线程调用执行函数`run`初始化线程之后立刻从任务队列里取任务执行。

```cpp
void Scheduler::start() {
    ...
    for(size_t i = 0 ; i < m_threadCount ; i++){
        m_threads[i].reset(new Thread(std::bind(&Scheduler::run , this)
        , m_name + "_" + std::to_string(i)));
        m_threadIds.push_back(m_threads[i]->getId());
    }
    ...
}
```

```cpp
void Scheduler::run() {
    ...
    if(radixun::GetThreadId() != m_rootThread){
        t_scheduler_fiber = Fiber::GetThis().get();//创建调度协程
    }
    //当任务队列为空时，代码会进idle协程
    Fiber::ptr idle_fiber(new Fiber(std::bind(&Scheduler::idle ,this)));
    //要执行的任务协程
    Fiber::ptr cb_fiber;
    //ft为要执行的任务
    FiberAndThread ft;
    while(true){
        ft.reset();
        //取任务
        {
            MutexType::Lock lock(m_mutex);
            auto it = m_fibers.begin();
            while(it != m_fibers.end()){
                ...
                ft = *it;
                m_fibers.erase(it);
                ...
                break;
            }
        }
        ...
        //做任务
        if(ft.fiber && ...){
            // resume协程，resume返回时，协程要么执行完了，要么半路yield了，总之这个任务就算完成了
            ft.fiber->swapIn();
            ...
        }else if(ft.cb) {
            //将ft函数打包作为ft.cb协程的执行...
            cb_fiber->swapIn();
            ...
        }else{
            // 任务队列空了，调度idle协程
            ...idle()
        }
    }
}
```

最后是`stop`方法，用来结束调度，当调用`stop`方法时调度协程才开始把之前`caller线程`添加的任务执行

```cpp
void Scheduler::stop() {
    ...
    /// 如果use caller，那只能由caller线程发起stop
    if(m_rootThread != -1){
        RADIXUN_ASSERT(GetThis() == this);
    }else{
        RADIXUN_ASSERT(GetThis() != this);
    }
    //提醒一下所有线程
    for(size_t i = 0 ; i < m_threadCount ; i ++){tickle();}
    if(m_rootFiber){tickle();}
    /// 在use caller情况下，调度器协程结束时，先切换到调度协程
    if(m_rootFiber){
        if(!stopping()){
            m_rootFiber->call();
        }
    }
    //调度协程执行完毕，回到主协程，阻塞等待所有线程执行完成
    std::vector<Thread::ptr> thrs;
    {
        MutexType::Lock lock(m_mutex);
        thrs.swap(m_threads);
    }
    for(auto& i :thrs){
        i->join();
    }
}
```

## IO协程调度器

---
IO协程调度器概述：

- IO协程调度器继承自协程调度器和定时器，封装了`epoll`相关io事件，拥有协程调度器的所有功能，并具体实现了`tickle`方法以及`idle`方法，让`dile`协程阻塞在`eopll_wait`上,`tickle`可触发`epoll_wait`的返回，还对特定描述符进行事件注册，当描述符事件触发时，唤醒对应的协程。

```cpp
class IOManager :public Scheduler , public TimerManager
```

对于IO协程调度来说，每次调度都包含一个三元组信息，`Socket`事件上线文类，调度器记录全部需要调度的三元组信息，其中描述符和事件类型用于`epoll_wait`，回调函数用于协程调度。这个三元组信息在源码上通过`FdContext`结构体来存储，在执行`epoll_wait`时通过`epoll_event`的私有数据指针`data.ptr`来保存`FdContext`结构体信息。

```cpp
//IO事件
enum Event{
    // 无事件
    NONE = 0x00,
    // 读事件(EPOLLIN)
    READ = 0x01,
    // 写事件(EPOLLOUT)
    WRITE = 0x4,
};

private:
// Socket事件上线文类(描述符-事件类型-回调函数)
struct FdContext{
    typedef Mutex MutexType;
    //事件上下文类
    struct EventContext {
        //执行事件回调的调度器
        Scheduler* scheduler = nullptr;
        //事件回调协程
        Fiber::ptr fiber;
        //事件回调函数
        std::function<void()> cb;
    };
    //获取事件上下文类
    EventContext& getContext(Event event);
    //重置事件上下文类
    void resetContext(EventContext& ctx);
    //触发事件
    void triggerEvent(Event event);

    // 读事件上下文
    EventContext read;
    // 写事件上下文
    EventContext write;
    // 事件关联的句柄
    int fd = 0;
    // 该fd添加了哪些事件的回调函数，或者说该fd关心哪些事件
    Event events = NONE;
    // 事件的Mutex
    MutexType mutex;
};
```

触发事件事件就是将回调函数加入调度器进行调度

```cpp
void IOManager::FdContext::triggerEvent(IOManager::Event event){
    ...//把事件从集合中删除
    //进行调度
    if(ctx.cb)ctx.scheduler->schedule(&ctx.cb);
    else ctx.scheduler->schedule(&ctx.fiber);
    ...
}
```

- **主要API**

用于对事件的操作，添加事件，删除事件，取消事件...

```cpp
public :
    IOManager(size_t threads = 1 , bool use_caller = true , const std::string& name = "");
    ~IOManager();

    int addEvent(int fd , Event event , std::function<void()> cb = nullptr);
    bool delEvent(int fd , Event event);
    bool cancelEvent(int fd , Event event);
    bool cancelAll(int fd);
    // 返回当前的IOManager
    static IOManager* GetThis();

protected:
    void tickle() override;
    bool stopping() override;
    void idle() override;
    void onTimerInsertedAtFront() override;
```

IOManager包含一个`epoll`实例的句柄`m_epfd`以及用于`tickle`的一对`pipe fd`，还有全部的`fd`上下文`m_fdContexts`

```cpp
// epoll 文件句柄
int m_epfd = 0;
// pipe 文件句柄，fd[0]读端，fd[1]写端
int m_tickleFds[2];
// 当前等待执行的IO事件数量
std::atomic<size_t> m_pendingEventCount = {0};
// IOManager的Mutex
RWMutexType m_mutex;
// socket事件上下文的容器
std::vector<FdContext*> m_fdContexts;
```

构造函数的主要任务即完成`epoll`的初始化

```cpp
IOManager::IOManager(size_t threads , bool use_caller , const std::string& name):Scheduler(threads , use_caller , name){
    m_epfd = epoll_create(5000);
    int rt = pipe(m_tickleFds);
    // 注册pipe读句柄的可读事件，用于tickle调度协程，通过epoll_event.data.fd保存描述符
    epoll_event event;
    memset(&event , 0 , sizeof(epoll_event));
    event.events = EPOLLIN | EPOLLET;//读数据+边缘触发
    event.data.fd = m_tickleFds[0];
    // 非阻塞方式，配合边缘触发
    rt = fcntl(m_tickleFds[0] , F_SETFL , O_NONBLOCK);
    // 将管道的读描述符加入epoll多路复用，如果管道可读，idle中的epoll_wait会返回
    rt = epoll_ctl(m_epfd , EPOLL_CTL_ADD , m_tickleFds[0] , &event);
    ...
    start();//直接开启线程池调度
}

IOManager::~IOManager(){
    stop();
    close(m_epfd);
    close(m_tickleFds[0]);
    close(m_tickleFds[1]);
    for(size_t i = 0 ; i > m_fdContexts.size() ; i ++){
        if(m_fdContexts[i]){
            delete m_fdContexts[i];
        }
    }
}
```

添加事件的具体实现，利用`epoll_ctl`添加事件并向事件上下文容器增加回调函数对象
其他操作大同小异

```cpp
int IOManager::addEvent(int fd , Event event , std::function<void()> cb){
    // 找到fd对应的FdContext，如果不存在，那就分配一个
    FdContext* fd_ctx = nullptr;
    ...
    fd_ctx = m_fdContexts[fd];
    ...
    // 将新的事件加入epoll_wait，使用epoll_event的私有指针存储FdContext的位置
    int op = fd_ctx->events ? EPOLL_CTL_MOD : EPOLL_CTL_ADD;
    epoll_event epevent;
    epevent.events = EPOLLET | fd_ctx->events | event;
    epevent.data.ptr = fd_ctx;   
    int rt = epoll_ctl(m_epfd, op, fd, &epevent);
    ...
    // 找到这个fd的event事件对应的EventContext，对其中的scheduler, cb, fiber进行赋值
    fd_ctx->events = (Event)(fd_ctx->events | event);
    FdContext::EventContext& event_ctx = fd_ctx->getContext(event);
    event_ctx.scheduler = Scheduler::GetThis();
    if(cb) {
        event_ctx.cb.swap(cb);
    } else { //null
        event_ctx.fiber = Fiber::GetThis();}
    return 0;
}
```

`tickle`时往管道里写入从而使`idle`线程从`epoll_wait`返回

```cpp
void IOManager::tickle(){
    ...
    int rt = write(m_tickleFds[1] , "T" , 1);
}
```

而从TImerManager重载的`onTimerInsertedAtFront`方法就是直接调用`tickle`代表有计时器插入到堆顶，从而先唤醒`idle`线程

```cpp
void IOManager::onTimerInsertedAtFront() {tickle();}
```

idle的实现

```cpp
void IOManager::idle() {
    epoll_event* events = new epoll_event[64]();
    std::shared_ptr<epoll_event> shared_events(events, [](epoll_event* ptr){delete[] ptr;});
 
    while (true) {
        ...
        // 阻塞在epoll_wait上，等待事件发生
        int rt = 0;
        do {
            ...
            rt = epoll_wait(m_epfd, events, 64, (int)next_timeout);
            if(rt < 0 && errno == EINTR) {}
            else {break;}
        } while(true);
        //查看定时器里有无任务时间到
        std::vector<std::function<void()> > cbs;
        listExpiredCb(cbs);
        if(!cbs.empty()) {
            schedule(cbs.begin(), cbs.end());
            cbs.clear();
        }
        // 遍历所有发生的事件，根据epoll_event的私有指针找到对应的FdContext，进行事件处理
        for (int i = 0; i < rt; ++i) {
            epoll_event& event = events[i];
            if(event.data.fd == m_tickleFds[0]) {
                uint8_t dummy;
                while(read(m_tickleFds[0], &dummy, 1) == 1);
                continue;
            }
            // 通过epoll_event的私有指针获取FdContext
            FdContext *fd_ctx = (FdContext *)event.data.ptr;
            FdContext::MutexType::Lock lock(fd_ctx->mutex);
            if (event.events & (EPOLLERR | EPOLLHUP)) {
                event.events |= (EPOLLIN | EPOLLOUT);
            }
            int real_events = NONE;
            if (event.events & EPOLLIN) {
                real_events |= READ;
            }
            if (event.events & EPOLLOUT) {
                real_events |= WRITE;
            }
            if ((fd_ctx->events & real_events) == NONE) {
                continue;
            }

            // 剔除已经发生的事件，将剩下的事件重新加入epoll_wait，
            // 如果剩下的事件为0，表示这个fd已经不需要关注了，直接从epoll中删除
            int left_events = (fd_ctx->events & ~real_events);
            int op          = left_events ? EPOLL_CTL_MOD : EPOLL_CTL_DEL;
            event.events    = EPOLLET | left_events;
            int rt2 = epoll_ctl(m_epfd, op, fd_ctx->fd, &event);
            
            // 处理已经发生的事件，也就是让调度器调度指定的函数或协程
            if (real_events & READ) {
                fd_ctx->triggerEvent(READ);
                --m_pendingEventCount;
            }
            if (real_events & WRITE) {
                fd_ctx->triggerEvent(WRITE);
                --m_pendingEventCount;
            }
        } // end for
        Fiber::ptr cur = Fiber::GetThis();
        auto raw_ptr   = cur.get();
        cur.reset();
        raw_ptr->swapOut();
    } // end while(true)
}
```
